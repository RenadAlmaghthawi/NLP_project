{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ler1Na-mAd5"
      },
      "source": [
        "# 1️⃣ Explore the train dataset, choose a representative sample\n",
        "---\n",
        "Your data is rather big, it'll be much faster if you begin experimenting with a smaller subset of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl6vWRtCJCDe",
        "outputId": "2311075f-9867-4fae-a553-af313fe3e383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   label                                              title  \\\n",
            "0      1  As U.S. budget fight looms, Republicans flip t...   \n",
            "1      1  U.S. military to accept transgender recruits o...   \n",
            "2      1  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
            "3      1  FBI Russia probe helped by Australian diplomat...   \n",
            "4      1  Trump wants Postal Service to charge 'much mor...   \n",
            "\n",
            "                                                text       subject  \\\n",
            "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
            "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
            "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
            "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
            "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
            "\n",
            "                 date  \n",
            "0  December 31, 2017   \n",
            "1  December 29, 2017   \n",
            "2  December 31, 2017   \n",
            "3  December 30, 2017   \n",
            "4  December 29, 2017   \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load only a sample of the dataset first\n",
        "texts = pd.read_csv(\"dataset/data.csv\")\n",
        "print(texts.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 39942 entries, 0 to 39941\n",
            "Data columns (total 5 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   label    39942 non-null  int64 \n",
            " 1   title    39942 non-null  object\n",
            " 2   text     39942 non-null  object\n",
            " 3   subject  39942 non-null  object\n",
            " 4   date     39942 non-null  object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 1.5+ MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(texts.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcfiCS3vLfuf",
        "outputId": "368521d1-25c7-4621-d983-8b2c21a4d1a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label      0\n",
            "title      0\n",
            "text       0\n",
            "subject    0\n",
            "date       0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(texts.isnull().sum())  # Count missing values per column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2Eq-O2rMDSa",
        "outputId": "8ee7b5e6-7ed9-4a29-e8cd-8ef44bd9427f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label\n",
            "1    19999\n",
            "0    19943\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(texts['label'].value_counts())  # Count fake (0) vs real (1) news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "subject\n",
              "Government News    0.0\n",
              "News               0.0\n",
              "left-news          0.0\n",
              "politics           0.0\n",
              "politicsNews       1.0\n",
              "worldnews          1.0\n",
              "Name: label, dtype: float64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts.groupby('subject').label.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqQemxQfOPDQ"
      },
      "source": [
        "### Clean Data:\n",
        "1. lowering & punctuation (custom_preprocessor)\n",
        "2. tokenization -> CountVectorizer\n",
        "3. remove stop words -> CountVectorizer (stop_words)\n",
        "4. lemmatize -> (custom preprocessor)\n",
        "5. removed rare words -> CountVectorizer (max_features)\n",
        "6. CountVectorize -> Bag of Words\n",
        "7. train test split\n",
        "8. classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bIQXVy7fO13e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\renad\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\renad\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\renad\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # Correct resource for tokenization\n",
        "nltk.download('wordnet')  # For lemmatization\n",
        "nltk.download('stopwords')  # For stopwords removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ERvWiXzzXOQR"
      },
      "outputs": [],
      "source": [
        "\n",
        "stop_words = set(stopwords.words('english'))  # Load stopwords list\n",
        "\n",
        "def custom_preprocessor(text):\n",
        "    \"\"\"Preprocess text by removing numbers, punctuation, and stopwords, then apply lemmatization.\"\"\"\n",
        "    \n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text.lower())  # Remove punctuation & convert to lowercase\n",
        "\n",
        "    tokens = text.split()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return ' '.join(lemmatized_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to create features from text data using TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def create_features(texts, max_features=3000, ngram_range=(1, 2)):\n",
        "    \"\"\"Convert text data into numerical features using TF-IDF Vectorizer.\"\"\"\n",
        "    \n",
        "    vectorizer = TfidfVectorizer(\n",
        "        stop_words='english',\n",
        "        max_features=max_features,  # Keep the 5000 most frequent words\n",
        "        ngram_range=ngram_range\n",
        "    )\n",
        "\n",
        "    tokens = vectorizer.fit_transform(texts)\n",
        "    return tokens, vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to apply the learned features on new text data\n",
        "def apply_features(texts, vectorizer):\n",
        "    \"\"\"Transform new text data using the trained CountVectorizer.\"\"\"\n",
        "    \n",
        "    tokens = vectorizer.transform(texts)\n",
        "    features = vectorizer.get_feature_names_out()\n",
        "    x_df = pd.DataFrame(tokens.toarray(), columns=features)\n",
        "    return x_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHDzJ7oRmKLy"
      },
      "source": [
        "# 2️⃣ Build a classical NLP model\n",
        "---\n",
        "You have several choices here\n",
        "\n",
        "How will you preprocess the data?\n",
        "\n",
        "Think about choice between stemming or lemmatization\n",
        "\n",
        "How many rare words should you remove?\n",
        "\n",
        "Should you use n-grams?\n",
        "\n",
        "Think what you want to do with different text fields?\n",
        "\n",
        "Maybe you can begin with one and check what the accuracy is.\n",
        "\n",
        "Think about the choice of classifier\n",
        "\n",
        "SVM, Logisitic Regression or Multinomial Bayes could be a good choice. Do you remember why?\n",
        "\n",
        "Select the best model and save it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4PvPaTYDWG55"
      },
      "outputs": [],
      "source": [
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    texts[['title', 'text', 'subject']],\n",
        "    texts['label'],\n",
        "    test_size=0.2,\n",
        "    random_state=62\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "gChnaVSWfpaE",
        "outputId": "973275a8-1a35-44b1-e261-3f2c435d947a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>subject</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10093</th>\n",
              "      <td>Obama to delay Spain visit until government is...</td>\n",
              "      <td>MADRID (Reuters) - U.S. President Barack Obama...</td>\n",
              "      <td>politicsNews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31028</th>\n",
              "      <td>GUY WHO MADE MILLIONS Selling “Science” To Kid...</td>\n",
              "      <td>William Stanford Nye or Bill Nye is an America...</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11148</th>\n",
              "      <td>U.S. tightens visa waiver rules for visitors a...</td>\n",
              "      <td>WASHINGTON (Reuters) - The United States on Th...</td>\n",
              "      <td>politicsNews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31828</th>\n",
              "      <td>WATCH! Anti-Trump Hag Gets Kicked Off Flight A...</td>\n",
              "      <td>Via: GATEWAY PUNDIT</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33475</th>\n",
              "      <td>RIDICULOUS! SECRET SERVICE Investigating “Poss...</td>\n",
              "      <td>Are they kidding? This was even a lead story o...</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36107</th>\n",
              "      <td>TUCKER ON COMEY’S FIRING: “Dictatorship by the...</td>\n",
              "      <td>TUCKER CARLSON Spoke out tonight on the firing...</td>\n",
              "      <td>Government News</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2163</th>\n",
              "      <td>Key in NAFTA talks is 'not tearing apart what ...</td>\n",
              "      <td>WASHINGTON (Reuters) - Mexico’s Economy Minist...</td>\n",
              "      <td>politicsNews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9809</th>\n",
              "      <td>Driven up the wall by Trump, Mexico looks to r...</td>\n",
              "      <td>MEXICO CITY (Reuters) - At first, Mexico’s gov...</td>\n",
              "      <td>politicsNews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15384</th>\n",
              "      <td>Judge orders Rosneft CEO Sechin to appear as w...</td>\n",
              "      <td>MOSCOW (Reuters) - A Russian judge on Wednesda...</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9746</th>\n",
              "      <td>White House says Biden's visit is a good indic...</td>\n",
              "      <td>WASHINGTON (Reuters) - The White House on Thur...</td>\n",
              "      <td>politicsNews</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>31953 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   title  \\\n",
              "10093  Obama to delay Spain visit until government is...   \n",
              "31028  GUY WHO MADE MILLIONS Selling “Science” To Kid...   \n",
              "11148  U.S. tightens visa waiver rules for visitors a...   \n",
              "31828  WATCH! Anti-Trump Hag Gets Kicked Off Flight A...   \n",
              "33475  RIDICULOUS! SECRET SERVICE Investigating “Poss...   \n",
              "...                                                  ...   \n",
              "36107  TUCKER ON COMEY’S FIRING: “Dictatorship by the...   \n",
              "2163   Key in NAFTA talks is 'not tearing apart what ...   \n",
              "9809   Driven up the wall by Trump, Mexico looks to r...   \n",
              "15384  Judge orders Rosneft CEO Sechin to appear as w...   \n",
              "9746   White House says Biden's visit is a good indic...   \n",
              "\n",
              "                                                    text          subject  \n",
              "10093  MADRID (Reuters) - U.S. President Barack Obama...     politicsNews  \n",
              "31028  William Stanford Nye or Bill Nye is an America...         politics  \n",
              "11148  WASHINGTON (Reuters) - The United States on Th...     politicsNews  \n",
              "31828                                Via: GATEWAY PUNDIT         politics  \n",
              "33475  Are they kidding? This was even a lead story o...         politics  \n",
              "...                                                  ...              ...  \n",
              "36107  TUCKER CARLSON Spoke out tonight on the firing...  Government News  \n",
              "2163   WASHINGTON (Reuters) - Mexico’s Economy Minist...     politicsNews  \n",
              "9809   MEXICO CITY (Reuters) - At first, Mexico’s gov...     politicsNews  \n",
              "15384  MOSCOW (Reuters) - A Russian judge on Wednesda...        worldnews  \n",
              "9746   WASHINGTON (Reuters) - The White House on Thur...     politicsNews  \n",
              "\n",
              "[31953 rows x 3 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCwi-7YbkSRZ",
        "outputId": "2fa07a05-bead-41be-e97e-67272230eeff"
      },
      "outputs": [],
      "source": [
        "# Convert Text into Numerical Representation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Concatenate text columns into a single feature \n",
        "x_train_text = x_train['title'] + \" \" + x_train['text'] \n",
        "x_test_text = x_test['title'] + \" \" + x_test['text'] \n",
        "\n",
        "\n",
        "X_train_vec, vectorizer = create_features(x_train_text, 3000 , (1,2))\n",
        "X_test_vec = vectorizer.transform(x_test_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh14GeiLAktb",
        "outputId": "296dad61-0e01-49ed-fc11-6933328cca61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔹 Model: Naive Bayes - Accuracy: 0.9432\n",
            "🔹 Model: Logistic Regression - Accuracy: 0.9881\n",
            "🔹 Model: SVM - Accuracy: 0.9945\n",
            "\n",
            "Best Model: SVM - Accuracy: 0.9945\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "models = {\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"SVM\": SVC(kernel='linear')\n",
        "}\n",
        "\n",
        "best_model = None\n",
        "best_accuracy = 0\n",
        "best_model_name = \"\"\n",
        "\n",
        "for name, model in models.items():\n",
        "    # Train model\n",
        "    model.fit(X_train_vec, y_train) \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_vec)  \n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)  \n",
        "\n",
        "    print(f\"🔹 Model: {name} - Accuracy: {accuracy:.4f}\")  \n",
        "\n",
        "    # Check if this model is the best so far\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model = model\n",
        "        best_model_name = name\n",
        "\n",
        "# Print the best model\n",
        "print(f\"\\nBest Model: {best_model_name} - Accuracy: {best_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### max_features , ngram_range Experments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   max_features ngram_range  accuracy\n",
            "0          1000      (1, 1)  0.989611\n",
            "1          1000      (1, 2)  0.991739\n",
            "2          3000      (1, 1)  0.992490\n",
            "3          3000      (1, 2)  0.994492\n",
            "4          5000      (1, 1)  0.992114\n",
            "5          5000      (1, 2)  0.993992\n"
          ]
        }
      ],
      "source": [
        "max_features_values = [1000, 3000, 5000]\n",
        "ngram_ranges = [(1,1), (1,2)]\n",
        "results = []\n",
        "\n",
        "for max_features in max_features_values:\n",
        "    for ngram_range in ngram_ranges:\n",
        "\n",
        "        X_train_vec, vectorizer = create_features(x_train_text, max_features, ngram_range)\n",
        "        X_test_vec = vectorizer.transform(x_test_text)\n",
        "        \n",
        "        # the best model\n",
        "        model = SVC(kernel='linear')\n",
        "        model.fit(X_train_vec, y_train)\n",
        "        \n",
        "        y_pred = model.predict(X_test_vec)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        \n",
        "        results.append({\n",
        "            \"max_features\": max_features,\n",
        "            \"ngram_range\": ngram_range,\n",
        "            \"accuracy\": accuracy\n",
        "        })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Increasing max_features and optimizing the ngram_range improved the model's accuracy, reaching a peak performance of 99.45% at max_features=3000 and ngram_range=(1,2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.pkl']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the best model and vectorizer\n",
        "import joblib\n",
        "\n",
        "joblib.dump(best_model, 'fake_news_classifier_svc.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK5jWT9hmdE1"
      },
      "source": [
        "# 3️⃣ Build a Word2Vec-based classifier\n",
        "---\n",
        "You have 2 options with regards to embeddings:\n",
        "\n",
        "You can create you own\n",
        "\n",
        "You can take ready-to-use embeddings (you can finetune it too)\n",
        "\n",
        "\n",
        "\n",
        "You have also a couple of options with regards to the model\n",
        "\n",
        "Calculate average/max over the document vector and use any classical classifier\n",
        "\n",
        "Use Conv1D Classifier or Kim's CNN architecture\n",
        "\n",
        "Note that if you use this, you'll have to make all documents the same length (the same number of tokens). You can do it either by padding or by truncating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drx0sFO3oTU0"
      },
      "source": [
        "### Option 2: Use Pre-trained Word2Vec Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "-opx_ts4mmoq",
        "outputId": "bebd3aa2-0dfa-4e11-c89d-b4eb747d0ce8"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "\n",
        "word2vec_model_google = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "    'GoogleNews-vectors-negative300.bin',\n",
        "    binary=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###\n",
        "We chose to use both classical classifier SVM and Kim’s CNN because each model processes data differently.\n",
        "\n",
        "* Classical classifiers require fixed-length inputs, so we used word embedding averaging to convert each text into a single fixed-length vector.\n",
        "* CNNs need to preserve word order, so we used word embedding sequences with padding to ensure all inputs have the same length.\n",
        "\n",
        "Each model has its own way of handling data, which is why we applied two different preprocessing methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVC classical classifier (average)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def embed_text(text, word2vec_model):\n",
        "    word_vectors = [word2vec_model[word] for word in text if word in word2vec_model]\n",
        "    if len(word_vectors) == 0:\n",
        "        return np.zeros(word2vec_model.vector_size)  \n",
        "    return np.mean(word_vectors, axis=0)  \n",
        "\n",
        "def embed_texts(texts, word2vec_model):\n",
        "    return np.array([embed_text(text, word2vec_model) for text in texts])\n",
        "\n",
        "x_train_embed = embed_texts(x_train_text, word2vec_model_google)\n",
        "x_test_embed = embed_texts(x_test_text, word2vec_model_google)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the Classical Classifier (SVC): 0.8564275879334085\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train an SVM classifier\n",
        "clf = SVC() \n",
        "clf.fit(x_train_embed, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_classical = clf.predict(x_test_embed)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy_classical = accuracy_score(y_test, y_pred_classical)\n",
        "print(f\"Accuracy of the Classical Classifier (SVC): {accuracy_classical}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* TF-IDF + SVM (Accuracy: 0.9945)\n",
        "\n",
        "Uses TF-IDF, which preserves word importance and distribution.\n",
        "Works well with SVM, leading to higher accuracy.\n",
        "\n",
        "* Word2Vec + SVM (Accuracy: 0.8597)\n",
        "\n",
        "Uses Word2Vec, averaging word embeddings into a single vector.\n",
        "Loses contextual details, reducing accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Kim's CNN architecture (padding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Conv1D, MaxPooling1D, Concatenate, Dropout, Flatten, Embedding\n",
        ")\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define constants\n",
        "MAX_VOCAB_SIZE = 10000  # Limit vocabulary size\n",
        "MAX_SEQUENCE_LENGTH = 100  # Limit max sequence length (prevents excessive memory usage)\n",
        "EMBEDDING_DIM = 300  # Word2Vec embedding size\n",
        "FILTER_SIZES = [3, 4, 5]  # Different filter sizes\n",
        "NUM_FILTERS = 100  # Number of filters per channel\n",
        "DROPOUT_RATE = 0.5  # Dropout rate\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(x_train_text)\n",
        "\n",
        "x_train_seq = tokenizer.texts_to_sequences(x_train_text)\n",
        "x_test_seq = tokenizer.texts_to_sequences(x_test_text)\n",
        "\n",
        "# Apply padding\n",
        "x_train_padded = pad_sequences(x_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "x_test_padded = pad_sequences(x_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "# Convert labels to categorical format\n",
        "num_classes = len(set(y_train))\n",
        "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "# Create embedding matrix using pre-trained Word2Vec\n",
        "def create_embedding_matrix(word2vec_model, tokenizer, vocab_size, embedding_dim):\n",
        "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index < vocab_size:\n",
        "            embedding_matrix[index] = word2vec_model[word] if word in word2vec_model else np.zeros(embedding_dim)\n",
        "    return embedding_matrix\n",
        "\n",
        "embedding_matrix = create_embedding_matrix(word2vec_model_google, tokenizer, MAX_VOCAB_SIZE, EMBEDDING_DIM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define CNN Model\n",
        "def yoon_kim_cnn():\n",
        "    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "\n",
        "    # Channel 1: Static pre-trained word2vec (non-trainable)\n",
        "    embedding_static = Embedding(\n",
        "        input_dim=MAX_VOCAB_SIZE, output_dim=EMBEDDING_DIM,\n",
        "        weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,\n",
        "        trainable=False, name='static_channel'\n",
        "    )(sequence_input)\n",
        "\n",
        "    # Channel 2: Non-static pre-trained word2vec (trainable)\n",
        "    embedding_non_static = Embedding(\n",
        "        input_dim=MAX_VOCAB_SIZE, output_dim=EMBEDDING_DIM,\n",
        "        weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH,\n",
        "        trainable=True, name='non_static_channel'\n",
        "    )(sequence_input)\n",
        "\n",
        "    # Channel 3: Random initialized embeddings (trainable)\n",
        "    embedding_random = Embedding(\n",
        "        input_dim=MAX_VOCAB_SIZE, output_dim=EMBEDDING_DIM,\n",
        "        input_length=MAX_SEQUENCE_LENGTH, trainable=True, name='random_channel'\n",
        "    )(sequence_input)\n",
        "\n",
        "    # Apply convolutions\n",
        "    conv_blocks = []\n",
        "    for embedding_layer, name in zip([embedding_static, embedding_non_static, embedding_random], [\"static\", \"non_static\", \"random\"]):\n",
        "        for filter_size in FILTER_SIZES:\n",
        "            conv = Conv1D(\n",
        "                filters=NUM_FILTERS // 3, kernel_size=filter_size,\n",
        "                padding='valid', activation='relu', strides=1,\n",
        "                name=f'conv_{name}_{filter_size}'\n",
        "            )(embedding_layer)\n",
        "            max_pool = MaxPooling1D(pool_size=MAX_SEQUENCE_LENGTH - filter_size + 1, name=f'maxpool_{name}_{filter_size}')(conv)\n",
        "            conv_blocks.append(max_pool)\n",
        "\n",
        "    # Concatenate all pooled features\n",
        "    z = Concatenate()(conv_blocks)\n",
        "    z = Flatten()(z)\n",
        "    z = Dropout(DROPOUT_RATE)(z)\n",
        "\n",
        "    outputs = Dense(num_classes, activation='softmax')(z)\n",
        "    model = Model(sequence_input, outputs)\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\renad\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ static_channel      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ non_static_channel  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ random_channel      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_static_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">29,733</span> │ static_channel[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_static_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">39,633</span> │ static_channel[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_static_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,533</span> │ static_channel[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_non_static_3   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">29,733</span> │ non_static_chann… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_non_static_4   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">39,633</span> │ non_static_chann… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_non_static_5   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,533</span> │ non_static_chann… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_random_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">29,733</span> │ random_channel[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_random_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">97</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">39,633</span> │ random_channel[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_random_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,533</span> │ random_channel[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_static_3    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_static_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_static_4    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_static_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_static_5    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_static_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_non_static… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_non_static_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_non_static… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_non_static_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_non_static… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_non_static_… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_random_3    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_random_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_random_4    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_random_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_random_5    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv_random_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ maxpool_static_3… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ maxpool_static_4… │\n",
              "│                     │                   │            │ maxpool_static_5… │\n",
              "│                     │                   │            │ maxpool_non_stat… │\n",
              "│                     │                   │            │ maxpool_non_stat… │\n",
              "│                     │                   │            │ maxpool_non_stat… │\n",
              "│                     │                   │            │ maxpool_random_3… │\n",
              "│                     │                   │            │ maxpool_random_4… │\n",
              "│                     │                   │            │ maxpool_random_5… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">297</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">596</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ static_channel      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m300\u001b[0m)  │  \u001b[38;5;34m3,000,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ non_static_channel  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m300\u001b[0m)  │  \u001b[38;5;34m3,000,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ random_channel      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m300\u001b[0m)  │  \u001b[38;5;34m3,000,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_static_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m33\u001b[0m)    │     \u001b[38;5;34m29,733\u001b[0m │ static_channel[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv1D\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_static_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m33\u001b[0m)    │     \u001b[38;5;34m39,633\u001b[0m │ static_channel[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv1D\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_static_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m33\u001b[0m)    │     \u001b[38;5;34m49,533\u001b[0m │ static_channel[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv1D\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_non_static_3   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m33\u001b[0m)    │     \u001b[38;5;34m29,733\u001b[0m │ non_static_chann… │\n",
              "│ (\u001b[38;5;33mConv1D\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_non_static_4   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m33\u001b[0m)    │     \u001b[38;5;34m39,633\u001b[0m │ non_static_chann… │\n",
              "│ (\u001b[38;5;33mConv1D\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_non_static_5   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m33\u001b[0m)    │     \u001b[38;5;34m49,533\u001b[0m │ non_static_chann… │\n",
              "│ (\u001b[38;5;33mConv1D\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_random_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m33\u001b[0m)    │     \u001b[38;5;34m29,733\u001b[0m │ random_channel[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv1D\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_random_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m97\u001b[0m, \u001b[38;5;34m33\u001b[0m)    │     \u001b[38;5;34m39,633\u001b[0m │ random_channel[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv1D\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv_random_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m33\u001b[0m)    │     \u001b[38;5;34m49,533\u001b[0m │ random_channel[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mConv1D\u001b[0m)            │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_static_3    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m33\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ conv_static_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_static_4    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m33\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ conv_static_4[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_static_5    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m33\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ conv_static_5[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_non_static… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m33\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ conv_non_static_… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_non_static… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m33\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ conv_non_static_… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_non_static… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m33\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ conv_non_static_… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_random_3    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m33\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ conv_random_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_random_4    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m33\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ conv_random_4[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ maxpool_random_5    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m33\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ conv_random_5[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m297\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ maxpool_static_3… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ maxpool_static_4… │\n",
              "│                     │                   │            │ maxpool_static_5… │\n",
              "│                     │                   │            │ maxpool_non_stat… │\n",
              "│                     │                   │            │ maxpool_non_stat… │\n",
              "│                     │                   │            │ maxpool_non_stat… │\n",
              "│                     │                   │            │ maxpool_random_3… │\n",
              "│                     │                   │            │ maxpool_random_4… │\n",
              "│                     │                   │            │ maxpool_random_5… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m297\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m297\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │        \u001b[38;5;34m596\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,357,293</span> (35.70 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,357,293\u001b[0m (35.70 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,357,293</span> (24.25 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,357,293\u001b[0m (24.25 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> (11.44 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,000,000\u001b[0m (11.44 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 121ms/step - accuracy: 0.9519 - loss: 0.1102 - val_accuracy: 0.9984 - val_loss: 0.0055\n",
            "Epoch 2/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 153ms/step - accuracy: 0.9987 - loss: 0.0051 - val_accuracy: 0.9990 - val_loss: 0.0045\n",
            "Epoch 3/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 164ms/step - accuracy: 0.9994 - loss: 0.0016 - val_accuracy: 0.9987 - val_loss: 0.0045\n",
            "Epoch 4/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 167ms/step - accuracy: 0.9999 - loss: 4.1401e-04 - val_accuracy: 0.9989 - val_loss: 0.0043\n",
            "Epoch 5/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 147ms/step - accuracy: 0.9998 - loss: 5.8657e-04 - val_accuracy: 0.9986 - val_loss: 0.0062\n",
            "Epoch 6/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 124ms/step - accuracy: 0.9999 - loss: 4.2188e-04 - val_accuracy: 0.9987 - val_loss: 0.0065\n",
            "Epoch 7/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 162ms/step - accuracy: 0.9998 - loss: 6.5029e-04 - val_accuracy: 0.9987 - val_loss: 0.0066\n",
            "Epoch 8/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 156ms/step - accuracy: 0.9999 - loss: 3.9691e-04 - val_accuracy: 0.9987 - val_loss: 0.0097\n",
            "Epoch 9/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 107ms/step - accuracy: 1.0000 - loss: 1.3111e-04 - val_accuracy: 0.9987 - val_loss: 0.0079\n",
            "Epoch 10/10\n",
            "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 106ms/step - accuracy: 1.0000 - loss: 3.0588e-05 - val_accuracy: 0.9989 - val_loss: 0.0088\n",
            "Test Accuracy: 0.9989\n"
          ]
        }
      ],
      "source": [
        "# Create and compile the model\n",
        "model = yoon_kim_cnn()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train_padded, y_train_cat, validation_data=(x_test_padded, y_test_cat), epochs=10, batch_size=64)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test_padded, y_test_cat, verbose=0)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Save trained CNN model\n",
        "model.save(\"cnn_model.keras\")\n",
        "\n",
        "# Save tokenizer\n",
        "with open(\"tokenizer.pkl\", \"wb\") as handle:\n",
        "    pickle.dump(tokenizer, handle)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
